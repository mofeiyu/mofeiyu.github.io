title: Wide & Deep
date: 2017-12-02 11:00
tags: [DNN]
categories: DNN
---

本文主要介绍Google的一篇神作，[Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792)

<!-- more -->

# 1. Why #

1. 传统线性模型：通过交叉特征获得记忆性，但是对于没有见过的特征，则表现比较差（泛化性差），泛化性则需要大量的特征工程。

    > Generalized Linear Models: Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort.

2. 深度模型：无需太多的特征工程，通过高维的稀疏特征学习到低维稠密的embedding，从而获得泛化性。然而，由于user-item的交互很稀疏，深度模型很容易过拟合，从而推荐低相关性的item。

    > With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. 

# 2. What #

提出Wide & Deep，通过联合训练深度模型和线性模型，从而获得更好的记忆性和泛化性。

> We present Wide & Deep learning—jointly trained wide linear models and deep neural networks—to combine the benefits of memorization and generalization for recommender systems.

# 3. How #

## 3.1. 推荐系统概览 ##

![Alt text](/img/dl/wide-and-deep/Overview of the recommender system.png)

由于YouTube拥有超过100万的apps，一次请求不可能对所有app进行打分，因此第一步是召回（retrieval），通过融合多种信号，结合机器学习和人工定义的规则，得到一个相对较小的候选集。第二步是对这个候选集进行排序打分，计算每个app被下载的概率。

> Since there are over a million apps in the database, it is intractable to exhaustively score every app for every query within the serving latency requirements (often O(10) milliseconds). Therefore, the first step upon receiving a query is retrieval. The retrieval system returns a short list of items that best match the query using various signals, usually a combination of machine-learned models and human-defined rules. After reducing the candidate pool, the ranking system ranks all items by their scores. The scores are usually P(y|x), the probability of a user action label y given the features x, including user features (e.g., country, language, demographics), contextual features (e.g., device, hour of the day, day of the week), and impression features (e.g., app age, historical statistics of an app). In this paper, we focus on the ranking model using the Wide & Deep learning framework.

## 3.2. Wide & Deep ##

![Alt text](/img/dl/wide-and-deep/The spectrum of Wide & Deep models.png)

### 3.2.1. The Wide Component ###

如上图左，wide侧其实就是一个线性模型，$ y = w^T x + b $，其中x和w都是一个d维的数组，x是特征向量，w是特征权重，b是bais。
特征包括原始输入特征和转换特征，其中交叉特征是最重要的转换特征之一。
何为转换特征？举个例子：假设gender和language都是单一特征，这时我们新增一个交叉特征"AND(gender=female, language=en)"，表示只有当"language=en"且"gender=female"时，才为1，否则为0。

> The wide component is a generalized linear model of the form $ y = w^T x + b $, as illustrated in figure above(left). y is the prediction, $ x = [x_1, x_2, ..., x_d] $ is a vector of d features, $ w = [w_1,w_2,...,w_d] $ are the model parameters and b is the bias. The feature set includes raw input features and transformed features. One of the most important transformations is the cross-product transformation, which is defined as:

> $ \phi(x) = \prod_{i=1}^{d}{x_i^{c_{ki}}}, c_{ki} \in \left(0, 1\right)$

> where $ c_{ki} $ is a boolean variable that is 1 if the i-th feature is part of the k-th transformation $ \phi_{k} $, and 0 otherwise. For binary features, a cross-product transformation (e.g., "AND(gender=female, language=en)") is 1 if and only if the constituent features ("gender=female" and "language=en") are all 1, and 0 otherwise. This captures the interactions between the binary features, and adds nonlinearity to the generalized linear model.

### 3.2.2. The Deep Component ###

如上图右，deep侧是一个前馈神经网络(feed-forward neural network)。对于一些分类特征，原始输入为字符串，这些稀疏、高维的分类特征，一般会先经过转换转为一个低维、稠密的实数型向量，该向量通常被称为embedding向量，维度一般在10~100之间。这些embedding向量被随机初始化，并作为神经网络的一层，跟第一层隐藏层进行全连接进行训练。

> The deep component is a feed-forward neural network, as shown in figure above(right). For categorical features, the original inputs are feature strings (e.g., “language=en”). Each of these sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. The dimensionality of the embeddings are usually on the order of O(10) to O(100). The embedding vectors are initialized randomly and then the values are trained to minimize the final loss function during model training. These low-dimensional dense embedding vectors are then fed into the hidden layers of a neural network in the forward pass. Specifically, each hidden layer performs the following computation:

> $ a^{(l+1)} = f(W^{(l)}a^{(l)} + b^{(l)}) $

> where l is the layer number and f is the activation function, often rectified linear units (ReLUs). $ a^{(l)} $, $ b^{(l)} $, and $ W^{(l)} $ are the activations, bias, and model weights at l-th layer.

怎么理解字符串转embedding向量？
比如gender假设有三种：male, female, other，我们使用one-hot编码：
~~~
male:   001
female: 010
other:  100
~~~
gender对应的embedding向量维度为16。我们可以认为一个维度3的输入与一个维度16的embedding向量进行了全连接，一共有3 X 16 = 48个权重，加上16个bais，一共64的参数。embedding向量又作为第一层隐藏层的输入，参与整个神经网络的训练，所有参数都会进行学习迭代。

比如下图，User Demographics，Device Class等分别映射到一个embedding向量，可简单理解为特征先进行hash分桶，然后经过一个全连接映射到一个低维、稠密的embedding向量。

![Alt text](/img/dl/wide-and-deep/Wide & Deep model structure for apps recommendation.png)

### 3.2.3. Joint Training of Wide & Deep Model ###

> The wide component and deep component are combined using a weighted sum of their output log odds as the prediction, which is then fed to one common logistic loss function for joint training. Note that there is a distinction between joint training and ensemble. In an ensemble, individual models are trained separately without knowing each other, and their predictions are combined only at inference time but not at training time. In contrast, joint training optimizes all parameters simultaneously by taking both the wide and deep part as well as the weights of their sum into account at training time. There are implications on model size too: For an ensemble, since the training is disjoint, each individual model size usually needs to be larger (e.g., with more features and transformations) to achieve reasonable accuracy for an ensemble to work. In comparison, for joint training the wide part only needs to complement the weaknesses of the deep part with a small number of cross-product feature transformations, rather than a full-size wide model.
Joint training of a Wide & Deep Model is done by backpropagating the gradients from the output to both the wide and deep part of the model simultaneously using mini-batch stochastic optimization. In the experiments, we used Follow-the-regularized-leader (FTRL) algorithm [3] with L1 regularization as the optimizer for the wide part of the model, and AdaGrad [1] for the deep part.

# 4. Tensorflow例子 #

~~~python
#coding=utf-8

import pandas as pd
import tensorflow as tf

g_Y = None

# 定义输入
def input_fn(data_path, num_epochs=1):
    global g_Y
    df_data = pd.read_csv(data_path)
    X, Y = df_data.iloc[:,:-1], df_data.label
    g_Y = Y
    return tf.estimator.inputs.pandas_input_fn(
        x=X, y=Y, batch_size=4, num_threads=1,
        shuffle=False, num_epochs=num_epochs)

def main():
    gender = tf.contrib.layers.sparse_column_with_keys(
        'gender', keys=['male', 'female', 'other'])
    language = tf.contrib.layers.sparse_column_with_keys(
        'language', keys=['chinese', 'japanese', 'english',
                          'hindi', 'german', 'other'])
    # 定义一个交叉特征
    gender_x_language = tf.contrib.layers.crossed_column(
        columns = [gender, language], hash_bucket_size = 100)
    # 定义embedding特征，维度16位
    gender_embedding = tf.contrib.layers.embedding_column(
        gender, dimension=16)
    # 定义embedding特征，维度32位
    language_embedding = tf.contrib.layers.embedding_column(
        language, dimension=32)
    # wide侧
    wide_cols = [gender_x_language]
    # deep侧
    deep_cols = [gender_embedding, language_embedding]
    model = tf.estimator.DNNLinearCombinedClassifier(
        linear_feature_columns = wide_cols,
        dnn_feature_columns = deep_cols,
        dnn_hidden_units = [16, 8])
    # 开始训练模型
    model.train(input_fn=input_fn('train.csv', num_epochs=1000),
                steps=1000)
    # 评估auc，准确率
    results = model.evaluate(input_fn=input_fn('evaluate.csv'))
    for key in sorted(results):
        print ("%s: %s" % (key, results[key]))
    # 评估打分
    results = model.predict(input_fn=input_fn('evaluate.csv'))
    for y, r in zip(g_Y, results):
        print ('label = %d, positive_ratio = %f'
               % (y, r['probabilities'][1]))

if __name__ == '__main__':
    main()
~~~

训练集，为了测试方便，构造数据时使gender=other和gender=female的label完全一致。
~~~
gender,language,label
male,chinese,1
female,chinese,1
male,english,1
female,english,0
other,english,0
female,japanese,1
other,japanese,1
male,hindi,0
other,hindi,0
male,german,0
female,german,1
other,other,1
~~~

评估集
~~~
gender,language,label
male,chinese,1
female,chinese,1
other,chinese,1
male,english,1
female,english,0
other,english,0
male,japanese,1
female,japanese,1
other,japanese,1
male,hindi,0
female,hindi,0
other,hindi,0
male,german,0
female,german,1
other,german,1
other,other,1
female,other,1
~~~

从结果来看，auc非常高，说明基本预测正确。
~~~
accuracy: 0.882353
accuracy_baseline: 0.647059
auc: 0.984848
auc_precision_recall: 0.99208
average_loss: 0.463514
global_step: 1000
label/mean: 0.647059
loss: 1.57595
prediction/mean: 0.613689
label = 1, positive_ratio = 0.774080
label = 1, positive_ratio = 0.746435
label = 1, positive_ratio = 0.699039
label = 1, positive_ratio = 0.701476
label = 0, positive_ratio = 0.579178
label = 0, positive_ratio = 0.529437
label = 1, positive_ratio = 0.554676
label = 1, positive_ratio = 0.650356
label = 1, positive_ratio = 0.691740
label = 0, positive_ratio = 0.384631
label = 0, positive_ratio = 0.468824
label = 0, positive_ratio = 0.402991
label = 0, positive_ratio = 0.433904
label = 1, positive_ratio = 0.711980
label = 1, positive_ratio = 0.651192
label = 1, positive_ratio = 0.731498
label = 1, positive_ratio = 0.721274
~~~


# 5. Tensorflow Wide & Deep源码剖析 #

待补充。
